---
title: "Conduct regular AI red teaming"
impact: MEDIUM
impactDescription: "general best practice"
tags: ai-security, security, llm-security, prompt-injection, model-poisoning
---

## Conduct regular AI red teaming

Conduct regular AI red teaming: using both manual expert testing and automated adversarial frameworks to continuously evaluate your AI system's resilience to attack.

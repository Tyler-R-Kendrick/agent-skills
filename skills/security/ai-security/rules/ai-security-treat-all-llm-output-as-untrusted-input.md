---
title: "Treat all LLM output as untrusted input"
impact: CRITICAL
impactDescription: "essential for correctness or security"
tags: ai-security, security, llm-security, prompt-injection, model-poisoning
---

## Treat all LLM output as untrusted input

Treat all LLM output as untrusted input: never pass LLM-generated content directly to interpreters, databases, APIs, or downstream systems without validation and encoding.

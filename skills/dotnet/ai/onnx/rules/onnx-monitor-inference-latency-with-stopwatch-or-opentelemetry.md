---
title: "Monitor inference latency with `Stopwatch` or OpenTelemetry..."
impact: MEDIUM
impactDescription: "general best practice"
tags: onnx, dotnet, ai, running-pre-trained-onnx-models-in-net, image-classification-inference, nlp-model-inference
---

## Monitor inference latency with `Stopwatch` or OpenTelemetry...

Monitor inference latency with `Stopwatch` or OpenTelemetry and set up alerts for p99 latency regressions that may indicate model size issues or execution provider misconfiguration.
